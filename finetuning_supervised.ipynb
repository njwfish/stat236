{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present a simple example of how to use fine-tuning with the hugging face setfit library.\n",
    "\n",
    "Make sure you run `pip install setfit` before running the code below.\n",
    "\n",
    "If you want to use a different library, you can use the hugging face transformers library, which is more comprehensive and flexible but also more complex and not designed for compute-restricted environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base sentiment examples\n",
    "positive_reviews = [\n",
    "    \"I love this product, it's amazing!\",\n",
    "    \"Great experience, highly recommend\",\n",
    "    \"Absolutely fantastic service\",\n",
    "    \"Best purchase I've ever made\",\n",
    "    \"The quality exceeded my expectations\",\n",
    "    \"Customer service was outstanding\",\n",
    "    \"Works perfectly for my needs\",\n",
    "    \"Really happy with this purchase\",\n",
    "    \"Great value for money\",\n",
    "    \"Would definitely buy again\"\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"This is terrible, don't buy it\",\n",
    "    \"Waste of money, very disappointed\",\n",
    "    \"Poor quality and bad customer service\",\n",
    "    \"Completely failed to meet expectations\",\n",
    "    \"Would not recommend to anyone\",\n",
    "    \"Save your money and avoid this\",\n",
    "    \"Terrible experience overall\",\n",
    "    \"Product broke after first use\",\n",
    "    \"Customer support was unhelpful\",\n",
    "    \"Not worth the price at all\"\n",
    "]\n",
    "\n",
    "def add_noise_to_text(text):\n",
    "    noise_words = [\n",
    "        \"today\", \"yesterday\", \"recently\", \"definitely\", \"absolutely\",\n",
    "        \"quite\", \"very\", \"extremely\", \"somewhat\", \"rather\"\n",
    "    ]\n",
    "    words = text.split()\n",
    "    # 50% chance to add a noise word at the start\n",
    "    if random.random() > 0.5:\n",
    "        words.insert(0, random.choice(noise_words))\n",
    "    # 50% chance to add a noise word at the end\n",
    "    if random.random() > 0.5:\n",
    "        words.append(random.choice(noise_words))\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Generate balanced dataset with variations\n",
    "texts = []\n",
    "labels = []\n",
    "num_examples = 50  # 50 examples of each sentiment\n",
    "\n",
    "for _ in range(num_examples):\n",
    "    # Add positive example\n",
    "    texts.append(add_noise_to_text(random.choice(positive_reviews)))\n",
    "    labels.append(1)\n",
    "    \n",
    "    # Add negative example\n",
    "    texts.append(add_noise_to_text(random.choice(negative_reviews)))\n",
    "    labels.append(0)\n",
    "\n",
    "# Convert labels to numpy array and shuffle both lists together\n",
    "labels = np.array(labels)\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts, labels = zip(*combined)\n",
    "texts = list(texts)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training document classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "/var/folders/6y/y7jr3zbd39b6fvmxt9lwswl00000gn/T/ipykernel_47035/1238241322.py:16: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e026cd88ae4780be1a746d8b067db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 3200\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b327e2084c46498386e95b41eb45372e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.3376, 'grad_norm': 2.461223840713501, 'learning_rate': 1.5e-06, 'epoch': 0.01}\n",
      "{'embedding_loss': 0.0974, 'grad_norm': 0.13390390574932098, 'learning_rate': 2.5e-05, 'epoch': 0.25}\n",
      "{'embedding_loss': 0.0027, 'grad_norm': 0.07220477610826492, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.5}\n",
      "{'embedding_loss': 0.0016, 'grad_norm': 0.07289315015077591, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.75}\n",
      "{'embedding_loss': 0.0014, 'grad_norm': 0.07542140036821365, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4d758bf25148068c3344adb0ed134e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_length` is `None`. Using the maximum acceptable length according to the current model body: 256.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 25.2003, 'train_samples_per_second': 126.983, 'train_steps_per_second': 7.936, 'train_loss': 0.026974876895546913, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e6a26359c94e4bbad2fae4fcfdcbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275c977f9e3749b3927b3c23dc5306fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Classification Accuracy: 1.00\n",
      "Text: 'This was above and beyond my expectations' -> Sentiment: Positive\n",
      "Text: 'I regret this purchase' -> Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate classification model\n",
    "print(\"Training document classification model...\")\n",
    "dataset = Dataset.from_dict({\n",
    "    \"text\": texts,\n",
    "    \"label\": labels\n",
    "})\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "model = SetFitModel.from_pretrained(\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    use_differentiable_head=True,\n",
    "    head_params={\"out_features\": len(set(labels))}, # Number of classes to predict\n",
    ")\n",
    "\n",
    "# Initialize trainer with improved training parameters\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    batch_size=16,\n",
    "    num_iterations=20,  \n",
    "    learning_rate=3e-5, # Large learning rate for this example, but this is not recommended for most cases\n",
    "    metric=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Make predictions\n",
    "preds_classification = model.predict(texts)\n",
    "acc_classification = accuracy_score(labels, preds_classification.cpu().numpy())\n",
    "print(f\"Document Classification Accuracy: {acc_classification:.2f}\")\n",
    "\n",
    "# Test sentiment\n",
    "new_texts = [\n",
    "    \"This was above and beyond my expectations\",\n",
    "    \"I regret this purchase\"\n",
    "]\n",
    "preds = model.predict(new_texts)\n",
    "for review, pred in zip(new_texts, preds):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    print(f\"Text: '{review}' -> Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
